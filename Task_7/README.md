# Task 6 - Support Vector Machines (SVM) Classification

## ğŸ“Œ Objective
The goal of this task is to **implement Support Vector Machines (SVM)** for **linear** and **non-linear classification**.  
We will also visualize decision boundaries, tune hyperparameters, and evaluate performance using cross-validation.

---

## ğŸ›  Tools & Libraries
- **Python 3.x**
- **Scikit-learn** â€“ For SVM models and dataset loading
- **NumPy** â€“ For numerical operations
- **Matplotlib** â€“ For data visualization

---

## ğŸ“‚ Dataset
We use the **Breast Cancer dataset** from `sklearn.datasets`.  
This is a binary classification dataset with features related to tumor measurements, used to classify tumors as **benign** or **malignant**.

---

## ğŸ“‹ Steps Followed

### 1ï¸âƒ£ Load and Prepare Dataset
- Load dataset from `sklearn.datasets`.
- Split into training and testing sets.
- Standardize features for better SVM performance.

### 2ï¸âƒ£ Train SVM with Linear Kernel
- Use `SVC(kernel='linear')` to train a linear classifier.
- Fit the model to training data.
- Make predictions and evaluate accuracy.

### 3ï¸âƒ£ Train SVM with RBF Kernel
- Use `SVC(kernel='rbf')` for non-linear classification.
- Fit the model and compare results with the linear kernel.

### 4ï¸âƒ£ Visualize Decision Boundary (2D Data)
- For demonstration, select **two features** from the dataset.
- Plot decision boundaries for both **linear** and **RBF** kernels.

### 5ï¸âƒ£ Hyperparameter Tuning
- Use `GridSearchCV` to tune parameters like **C** (regularization) and **gamma** (RBF kernel parameter).
- Select the best model based on accuracy.

### 6ï¸âƒ£ Cross-Validation
- Perform **k-fold cross-validation** to evaluate the modelâ€™s generalization performance.

---

## ğŸ“Š Results
- **Linear Kernel** performs well when data is linearly separable.
- **RBF Kernel** handles complex non-linear patterns better.
- Hyperparameter tuning significantly improves model accuracy.

---
